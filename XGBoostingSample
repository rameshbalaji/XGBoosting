setwd("C:/Ramesh/R Programs/Walmart/XGBoost")
wmttraindata <- read.csv(file="train.csv")
wmttestdata <- read.csv(file="test.csv")



wmtorgtraindata <- wmttraindata

train.indeces <- sample(1:nrow(wmttraindata),5000)
wmttraindata <- wmttraindata[train.indeces,]
nrow(wmttraindata)
nrow(wmttestdata)

library(car)

wmttraindata <- na.omit(wmttraindata)
wmttestdata <- na.omit(wmttestdata)

y=recode(wmttraindata$TripType,"999=0;30=1;26=2;8=3;35=4;41=5;21=6;6=7;
42=8;7=9;9=10;39=11;25=12;38=13;15=14;36=15;20=16;37=17;32=18;40=19;5=20;
3=21;4=22;24=23;33=24;43=25;31=26;27=27;34=28;18=29;29=30;44=31;19=32;
23=33;22=34;28=35;14=36;12=37")



outcome.org <- wmttraindata$TripType
num.class <- length(unique(wmttraindata$TripType))
num.class


## Train data
num.weekday = length(levels(wmttraindata$Weekday))
levels(wmttraindata$Weekday) = 1:num.weekday
head(wmttraindata$Weekday)


num.departmentdesc = length(levels(wmttraindata$DepartmentDescription))
levels(wmttraindata$DepartmentDescription) = 1:num.departmentdesc
head(wmttraindata$DepartmentDescription)

## Test data
num.weekday = length(levels(wmttestdata$Weekday))
levels(wmttestdata$Weekday) = 1:num.weekday
head(wmttestdata$Weekday)

num.departmentdesc = length(levels(wmttestdata$DepartmentDescription))
levels(wmttestdata$DepartmentDescription) = 1:num.departmentdesc
head(wmttestdata$DepartmentDescription)


library(xgboost)
library(caret)
library(corrplot)
library(Rtsne)
library(stats)
library(knitr)
library(ggplot2)

wmttraindata$TripType = NULL

wmttraindata.matrix = as.matrix(wmttraindata)
mode(wmttraindata.matrix) = "numeric"
wmttestdata.matrix = as.matrix(wmttestdata)
mode(wmttestdata.matrix) = "numeric"
y = as.matrix(as.integer(y))

set.seed(1234)
## xgboost parameters


param <- list("objective" = "multi:softprob",    # multiclass classification 
              "num_class" = num.class,    # number of classes 
              "eval_metric" = "merror",    # evaluation metric 
              "nthread" = 8,   # number of threads to be used 
              "max_depth" = 300,    # maximum depth of tree 
              "eta" = 0.3,    # step size shrinkage 
              "gamma" = 0,    # minimum loss reduction 
              "subsample" = 1,    # part of data instances to grow tree 
              "colsample_bytree" = 1,  # subsample ratio of columns when constructing each tree 
             "min_child_weight" = 12  # minimum sum of instance weight needed in a child 
            )

nround.cv = 300

system.time( bst.cv <- xgb.cv(param=param, data=wmttraindata.matrix, label=y, 
              nfold=4, nrounds=nround.cv, prediction=TRUE, verbose=FALSE) )

tail(bst.cv$dt) 
min.merror.idx = which.min(bst.cv$dt[, test.merror.mean]) 
min.merror.idx 
bst.cv$dt[min.merror.idx,]

# get CV's prediction decoding
pred.cv = matrix(bst.cv$pred, nrow=length(bst.cv$pred)/num.class, ncol=num.class)
pred.cv = max.col(pred.cv, "last")
# confusion matrix
confusionMatrix(factor(y+1), factor(pred.cv))

# real model fit training, with full data
system.time( bst <- xgboost(param=param, data=wmttraindata.matrix, label=y, 
                           nrounds=min.merror.idx, verbose=0) )
summary(bst.cv)

